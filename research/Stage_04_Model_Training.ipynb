{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3b60f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a5e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from src.vision_Transformer.constants import *\n",
    "from src.vision_Transformer.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f171e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen = True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir : Path\n",
    "    train_accuracy : Path\n",
    "    train_loss : Path\n",
    "    data_dir : Path\n",
    "\n",
    "\n",
    "    batch_size : int\n",
    "    epochs : int \n",
    "    learning_rate : float\n",
    "    patch_size : int\n",
    "    num_classes : int\n",
    "    image_size : int \n",
    "    channels : int\n",
    "    embed_dim : int\n",
    "    num_heads: int\n",
    "    depth : int\n",
    "    mlp_dim : int\n",
    "    dropout_rate : float\n",
    "    weight_decay : float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76b75fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_file_path = CONFIG_FILE_PATH ,params_file_path = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_file_path)\n",
    "        self.params = read_yaml(params_file_path)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.TrainingArguments\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir= config.root_dir,\n",
    "            train_accuracy = config.train_accuracy,\n",
    "            train_loss = config.train_loss,\n",
    "            data_dir = config.data_dir,\n",
    "\n",
    "            batch_size= params.BATCH_SIZE,\n",
    "            epochs = params.EPOCHS,\n",
    "            learning_rate = params.LEARNING_RATE,\n",
    "            patch_size = params.PATCH_SIZE,\n",
    "            num_classes = params.NUM_CLASSES,\n",
    "            image_size = params.IMAGE_SIZE,\n",
    "            channels = params.CHANNELS,\n",
    "            embed_dim  = params.EMBED_DIM,\n",
    "            num_heads = params.NUM_HEADS,\n",
    "            depth = params.DEPTH,\n",
    "            mlp_dim = params.MLP_DIM,\n",
    "            dropout_rate = params.DROPOUT_RATE,\n",
    "            weight_decay  = params.WEIGHT_DECAY\n",
    "        )\n",
    "        return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7494d4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5879c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.vision_Transformer.logging import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab48ed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self , config : ModelTrainerConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def data_augmentation(self):\n",
    "        self.after_transforms = transforms.Compose([\n",
    "            transforms.RandomCrop(32 , padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.2 ,contrast= 0.2, saturation=0.2 , hue=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5]*3 , std = [0.5]*3)\n",
    "        ])\n",
    "    \n",
    "    def transformed_dataset(self):\n",
    "        transformed_train_dataset = datasets.CIFAR10(\n",
    "            root = self.config.data_dir,\n",
    "            train = True,\n",
    "            download= False,\n",
    "            transform= self.after_transforms,\n",
    "        )\n",
    "        logger.info(f\"Train Dataset Transformed Successfully\")\n",
    "        print(f\"Train Dataset Transformed Successfully\")\n",
    "\n",
    "        return transformed_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "83e24aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-09 13:37:57,183 : INFO : common  : yaml file config\\config.yaml was read succesfully]\n",
      "[2025-08-09 13:37:57,188 : INFO : common  : yaml file params.yaml was read succesfully]\n",
      "[2025-08-09 13:37:57,189 : INFO : common  : Created directory at : artifacts]\n",
      "[2025-08-09 13:37:57,192 : INFO : common  : Created directory at : artifacts/model/trained_model]\n",
      "[2025-08-09 13:37:57,899 : INFO : 3539883775  : Train Dataset Transformed Successfully]\n",
      "Train Dataset Transformed Successfully\n"
     ]
    }
   ],
   "source": [
    "config = ConfigurationManager()\n",
    "model_trainer_config = config.get_model_trainer_config()\n",
    "\n",
    "data_transfromation = DataTransformation(model_trainer_config)\n",
    "data_transfromation.data_augmentation()\n",
    "\n",
    "train_dataset= data_transfromation.transformed_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dfdd4b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.vision_Transformer.Components.ViT_Component.Vision_Transformer_Class import Vision_Transformer_Class\n",
    "from tqdm.auto import tqdm\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ccdd1c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_trainer:\n",
    "    def __init__(self, config: ModelTrainerConfig , train_dataset):\n",
    "        self.config = config\n",
    "        \n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        self.train_dataset = train_dataset\n",
    "\n",
    "        self.train_loader = DataLoader(train_dataset , batch_size = self.config.batch_size ,shuffle = True ,pin_memory= True)\n",
    "\n",
    "        self.model = Vision_Transformer_Class(\n",
    "            image_size = self.config.image_size,\n",
    "            patch_size = self.config.patch_size,\n",
    "            in_channels = self.config.channels,\n",
    "            num_classes = self.config.num_classes,\n",
    "            embed_dim = self.config.embed_dim,\n",
    "            num_heads = self.config.num_heads,  # 8\n",
    "            depth = self.config.depth,      # 6\n",
    "            mlp_dim = self.config.mlp_dim,\n",
    "            dropout_rate = self.config.dropout_rate\n",
    "        ).to(self.device)\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "        self.criterion = nn.CrossEntropyLoss(label_smoothing= 0.1)\n",
    "        self.optimizer = optim.AdamW(self.model.parameters() , lr = float(self.config.learning_rate) , weight_decay= self.config.weight_decay)\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer= self.optimizer , T_max= self.config.epochs)\n",
    "    \n",
    "    # ---------------------------------------------------------------------------\n",
    "    def show_model(self):\n",
    "        print(\"\\n\\n------------------------------->Model Configuration<------------------------------------\")\n",
    "        print(\"\\n\", self.model)\n",
    "\n",
    "        print(\"\\n\\n\\n\", self.device)\n",
    "        \n",
    "    def train(self):\n",
    "        #set the model to the training mode \n",
    "        self.model.train()\n",
    "        \n",
    "\n",
    "        total_loss , correct_prediction = 0 , 0\n",
    "\n",
    "        for x , y in self.train_loader:\n",
    "            x , y = x.to(self.device) , y.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            output = self.model(x)\n",
    "\n",
    "            loss = self.criterion(output , y)\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "\n",
    "            correct_prediction += (output.argmax(1) == y).sum().item()\n",
    "\n",
    "        return total_loss/len(self.train_loader.dataset) , correct_prediction / len(self.train_loader.dataset)\n",
    "    \n",
    "\n",
    "    def model_train_pipeline(self):\n",
    "        train_logs = []\n",
    "\n",
    "        train_accuracy_file = self.config.train_accuracy\n",
    "        os.makedirs(os.path.dirname(train_accuracy_file) , exist_ok= True)\n",
    "\n",
    "\n",
    "        for epoch in tqdm(range(self.config.epochs)):\n",
    "\n",
    "            train_loss , train_acc = self.train()\n",
    "\n",
    "            log = {\n",
    "            \"epoch\": f\" {epoch+1} /{self.config.epochs}\",\n",
    "            \"train_loss\": f\" {train_loss:.4f}\",\n",
    "            \"train_acc\": f\" {train_acc:.4f}%\"\n",
    "            }\n",
    "\n",
    "            train_logs.append(log)\n",
    "\n",
    "            print(f\"Epoch: {epoch+1}/{self.config.epochs}, Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f}%\")\n",
    "\n",
    "        with open(train_accuracy_file ,'w') as f:\n",
    "                json.dump(log , f)\n",
    "\n",
    "\n",
    "    def save_model(self):\n",
    "        # ===============================\n",
    "        # 1. Save model parameters only (state_dict)\n",
    "        # ===============================\n",
    "\n",
    "        state_dict_dir = self.config.root_dir\n",
    "        os.makedirs(state_dict_dir , exist_ok= True)\n",
    "\n",
    "        state_dict_path = os.path.join(state_dict_dir , \"model_weights.pth\")\n",
    "        torch.save(self.model.state_dict() , state_dict_path)\n",
    "        print(f\"Saved model weights (state_dict) to: {state_dict_path}\")  \n",
    "\n",
    "                \n",
    "        # ===============================\n",
    "        # 2. Save full model (architecture + parameters)\n",
    "        # ===============================  \n",
    "        full_model_dir = self.config.root_dir\n",
    "        os.makedirs(full_model_dir , exist_ok= True)\n",
    "        \n",
    "        full_model_path = os.path.join(full_model_dir , \"complete_model.pth\")\n",
    "        torch.save(self.model , full_model_path)\n",
    "        print(f\"Saved full model to: {full_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d4cbfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-09 14:18:59,400 : INFO : common  : yaml file config\\config.yaml was read succesfully]\n",
      "[2025-08-09 14:18:59,406 : INFO : common  : yaml file params.yaml was read succesfully]\n",
      "[2025-08-09 14:18:59,408 : INFO : common  : Created directory at : artifacts]\n",
      "[2025-08-09 14:18:59,412 : INFO : common  : Created directory at : artifacts/model/trained_model]\n",
      "[2025-08-09 14:19:00,134 : INFO : 3539883775  : Train Dataset Transformed Successfully]\n",
      "Train Dataset Transformed Successfully\n",
      "\n",
      "\n",
      "------------------------------->Model Configuration<------------------------------------\n",
      "\n",
      " Vision_Transformer_Class(\n",
      "  (patch_embedding): PatchEmbedding(\n",
      "    (projection): Conv2d(3, 256, kernel_size=(4, 4), stride=(4, 4))\n",
      "  )\n",
      "  (encoder_layer): Sequential(\n",
      "    (0): Transformer_Encoder_Layer(\n",
      "      (normalization_layer_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (multi_head_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=256, out_features=768, bias=True)\n",
      "        (fc2): Linear(in_features=768, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (normalization_layer_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): Transformer_Encoder_Layer(\n",
      "      (normalization_layer_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (multi_head_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=256, out_features=768, bias=True)\n",
      "        (fc2): Linear(in_features=768, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (normalization_layer_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): Transformer_Encoder_Layer(\n",
      "      (normalization_layer_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (multi_head_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=256, out_features=768, bias=True)\n",
      "        (fc2): Linear(in_features=768, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (normalization_layer_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): Transformer_Encoder_Layer(\n",
      "      (normalization_layer_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (multi_head_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=256, out_features=768, bias=True)\n",
      "        (fc2): Linear(in_features=768, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (normalization_layer_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (4): Transformer_Encoder_Layer(\n",
      "      (normalization_layer_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (multi_head_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=256, out_features=768, bias=True)\n",
      "        (fc2): Linear(in_features=768, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (normalization_layer_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (5): Transformer_Encoder_Layer(\n",
      "      (normalization_layer_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (multi_head_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=256, out_features=768, bias=True)\n",
      "        (fc2): Linear(in_features=768, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (normalization_layer_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (6): Transformer_Encoder_Layer(\n",
      "      (normalization_layer_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (multi_head_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=256, out_features=768, bias=True)\n",
      "        (fc2): Linear(in_features=768, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (normalization_layer_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (7): Transformer_Encoder_Layer(\n",
      "      (normalization_layer_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (multi_head_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=256, out_features=768, bias=True)\n",
      "        (fc2): Linear(in_features=768, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (normalization_layer_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (normalization_layer): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (classification_head): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      " cuda\n",
      "\n",
      "-----------------------Model Training----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [02:26<58:29, 146.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/25, Train loss: 2.0093, Train acc: 0.2919%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [03:33<1:25:35, 213.98s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m model_trainer\u001b[38;5;241m.\u001b[39mshow_model()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-----------------------Model Training----------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m \u001b[43mmodel_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_train_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-----------------------Model Training Completed----------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m model_trainer\u001b[38;5;241m.\u001b[39msave_model()\n",
      "Cell \u001b[1;32mIn[42], line 72\u001b[0m, in \u001b[0;36mModel_trainer.model_train_pipeline\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     67\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(train_accuracy_file) , exist_ok\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mepochs)):\n\u001b[1;32m---> 72\u001b[0m     train_loss , train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m     log \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m /\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     78\u001b[0m     }\n\u001b[0;32m     80\u001b[0m     train_logs\u001b[38;5;241m.\u001b[39mappend(log)\n",
      "Cell \u001b[1;32mIn[42], line 43\u001b[0m, in \u001b[0;36mModel_trainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     41\u001b[0m total_loss , correct_prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m , \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x , y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader:\n\u001b[0;32m     44\u001b[0m     x , y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) , y\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torchvision\\datasets\\cifar.py:119\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    116\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torchvision\\transforms\\transforms.py:1277\u001b[0m, in \u001b[0;36mColorJitter.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fn_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m contrast_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m     img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madjust_contrast(img, contrast_factor)\n\u001b[1;32m-> 1277\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mfn_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m \u001b[38;5;129;01mand\u001b[39;00m saturation_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1278\u001b[0m     img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madjust_saturation(img, saturation_factor)\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fn_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hue_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    data_transfromation = DataTransformation(model_trainer_config)\n",
    "    data_transfromation.data_augmentation()\n",
    "\n",
    "    train_dataset = data_transfromation.transformed_dataset()\n",
    "\n",
    "    model_trainer = Model_trainer(model_trainer_config , train_dataset=train_dataset )\n",
    "\n",
    "    model_trainer.show_model()\n",
    "\n",
    "    print(\"\\n-----------------------Model Training----------------------------\")\n",
    "    model_trainer.model_train_pipeline()\n",
    "\n",
    "    print(\"\\n-----------------------Model Training Completed----------------------------\")\n",
    "    model_trainer.save_model()\n",
    "\n",
    "except Exception as e:\n",
    "  raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a03c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccbcbf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec644d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c7271f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3682f9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
