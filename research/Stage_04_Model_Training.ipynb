{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3b60f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a5e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from src.vision_Transformer.constants import *\n",
    "from src.vision_Transformer.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f171e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen = True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir : Path\n",
    "    trained_model : str\n",
    "    train_accuracy : Path\n",
    "    train_loss : Path\n",
    "    data_dir : Path\n",
    "\n",
    "\n",
    "    batch_size : int\n",
    "    epochs : int \n",
    "    learning_rate : float\n",
    "    patch_size : int\n",
    "    num_classes : int\n",
    "    image_size : int \n",
    "    channels : int\n",
    "    embed_dim : int\n",
    "    num_heads: int\n",
    "    depth : int\n",
    "    mlp_dim : int\n",
    "    dropout_rate : float\n",
    "    weight_decay : float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76b75fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_file_path = CONFIG_FILE_PATH ,params_file_path = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_file_path)\n",
    "        self.params = read_yaml(params_file_path)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.TrainingArguments\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir= config.root_dir,\n",
    "            trained_model = config.trained_model,\n",
    "            train_accuracy = config.train_accuracy,\n",
    "            train_loss = config.train_loss,\n",
    "            data_dir = config.data_dir,\n",
    "\n",
    "            batch_size= params.BATCH_SIZE,\n",
    "            epochs = params.EPOCHS,\n",
    "            learning_rate = params.LEARNING_RATE,\n",
    "            patch_size = params.PATCH_SIZE,\n",
    "            num_classes = params.NUM_CLASSES,\n",
    "            image_size = params.IMAGE_SIZE,\n",
    "            channels = params.CHANNELS,\n",
    "            embed_dim  = params.EMBED_DIM,\n",
    "            num_heads = params.NUM_HEADS,\n",
    "            depth = params.DEPTH,\n",
    "            mlp_dim = params.MLP_DIM,\n",
    "            dropout_rate = params.DROPOUT_RATE,\n",
    "            weight_decay  = params.WEIGHT_DECAY\n",
    "        )\n",
    "        return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7494d4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5879c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.vision_Transformer.logging import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab48ed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self , config : ModelTrainerConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def data_augmentation(self):\n",
    "        self.after_transforms = transforms.Compose([\n",
    "            transforms.RandomCrop(32 , padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.2 ,contrast= 0.2, saturation=0.2 , hue=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5]*3 , std = [0.5]*3)\n",
    "        ])\n",
    "    \n",
    "    def transformed_dataset(self):\n",
    "        transformed_train_dataset = datasets.CIFAR10(\n",
    "            root = self.config.data_dir,\n",
    "            train = True,\n",
    "            download= False,\n",
    "            transform= self.after_transforms,\n",
    "        )\n",
    "        logger.info(f\"Train Dataset Transformed Successfully\")\n",
    "        print(f\"Train Dataset Transformed Successfully\")\n",
    "\n",
    "        transformed_test_dataset = datasets.CIFAR10(\n",
    "            root = self.config.data_dir,\n",
    "            train = False,\n",
    "            download= False,\n",
    "            transform= self.after_transforms,\n",
    "        )\n",
    "        logger.info(f\"Test Dataset Transformed Successfully\")\n",
    "        print(f\"Test Dataset Transformed Successfully\")\n",
    "\n",
    "        return transformed_train_dataset , transformed_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83e24aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-08 15:53:16,354 : INFO : common  : yaml file config\\config.yaml was read succesfully]\n",
      "[2025-08-08 15:53:16,362 : INFO : common  : yaml file params.yaml was read succesfully]\n",
      "[2025-08-08 15:53:16,364 : INFO : common  : Created directory at : artifacts]\n",
      "[2025-08-08 15:53:16,364 : INFO : common  : Created directory at : artifacts/model]\n",
      "[2025-08-08 15:53:17,000 : INFO : 4111442926  : Train Dataset Transformed Successfully]\n",
      "Train Dataset Transformed Successfully\n",
      "[2025-08-08 15:53:17,492 : INFO : 4111442926  : Test Dataset Transformed Successfully]\n",
      "Test Dataset Transformed Successfully\n"
     ]
    }
   ],
   "source": [
    "config = ConfigurationManager()\n",
    "model_trainer_config = config.get_model_trainer_config()\n",
    "\n",
    "data_transfromation = DataTransformation(model_trainer_config)\n",
    "data_transfromation.data_augmentation()\n",
    "\n",
    "train_dataset , test_dataset = data_transfromation.transformed_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfdd4b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.vision_Transformer.Components.ViT_Component.Vision_Transformer_Class import Vision_Transformer_Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ccdd1c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_trainer:\n",
    "    def __init__(self, config: ModelTrainerConfig , train_dataset , test_dataset):\n",
    "        self.config = config\n",
    "        \n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "\n",
    "        self.train_loader = DataLoader(train_dataset , batch_size = self.config.batch_size ,shuffle = True ,pin_memory= True)\n",
    "\n",
    "        self.test_loader =  DataLoader(test_dataset , batch_size = self.config.batch_size ,shuffle = False ,pin_memory= True)\n",
    "\n",
    "        self.model = Vision_Transformer_Class(\n",
    "            image_size = self.config.image_size,\n",
    "            patch_size = self.config.patch_size,\n",
    "            in_channels = self.config.channels,\n",
    "            num_classes = self.config.num_classes,\n",
    "            embed_dim = self.config.embed_dim,\n",
    "            num_heads = self.config.num_heads,  # 8\n",
    "            depth = self.config.depth,      # 6\n",
    "            mlp_dim = self.config.mlp_dim,\n",
    "            dropout_rate = self.config.dropout_rate\n",
    "        ).to(self.device)\n",
    "    \n",
    "    # ---------------------------------------------------------------------------\n",
    "    def show_model(self):\n",
    "        print(\"\\n\\n------------------------------->Model Configuration<------------------------------------\")\n",
    "        print(\"\\n\", self.model)\n",
    "        \n",
    "    def train(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d4cbfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-08 15:54:57,959 : INFO : common  : yaml file config\\config.yaml was read succesfully]\n",
      "[2025-08-08 15:54:57,963 : INFO : common  : yaml file params.yaml was read succesfully]\n",
      "[2025-08-08 15:54:57,965 : INFO : common  : Created directory at : artifacts]\n",
      "[2025-08-08 15:54:57,966 : INFO : common  : Created directory at : artifacts/model]\n",
      "[2025-08-08 15:54:58,650 : INFO : 4111442926  : Train Dataset Transformed Successfully]\n",
      "Train Dataset Transformed Successfully\n",
      "[2025-08-08 15:54:59,276 : INFO : 4111442926  : Test Dataset Transformed Successfully]\n",
      "Test Dataset Transformed Successfully\n",
      "\n",
      "\n",
      "------------------------------->Model Configuration<------------------------------------\n",
      "\n",
      " Vision_Transformer_Class(\n",
      "  (patch_embedding): PatchEmbedding(\n",
      "    (projection): Conv2d(3, 256, kernel_size=(4, 4), stride=(4, 4))\n",
      "  )\n",
      "  (encoder_layer): Sequential(\n",
      "    (0): Transformer_Encoder_Layer(\n",
      "      (normalization_layer_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (multi_head_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=256, out_features=768, bias=True)\n",
      "        (fc2): Linear(in_features=768, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (normalization_layer_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): Transformer_Encoder_Layer(\n",
      "      (normalization_layer_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (multi_head_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=256, out_features=768, bias=True)\n",
      "        (fc2): Linear(in_features=768, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (normalization_layer_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): Transformer_Encoder_Layer(\n",
      "      (normalization_layer_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (multi_head_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=256, out_features=768, bias=True)\n",
      "        (fc2): Linear(in_features=768, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (normalization_layer_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): Transformer_Encoder_Layer(\n",
      "      (normalization_layer_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (multi_head_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=256, out_features=768, bias=True)\n",
      "        (fc2): Linear(in_features=768, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (normalization_layer_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (4): Transformer_Encoder_Layer(\n",
      "      (normalization_layer_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (multi_head_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=256, out_features=768, bias=True)\n",
      "        (fc2): Linear(in_features=768, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (normalization_layer_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (5): Transformer_Encoder_Layer(\n",
      "      (normalization_layer_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (multi_head_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=256, out_features=768, bias=True)\n",
      "        (fc2): Linear(in_features=768, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (normalization_layer_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (6): Transformer_Encoder_Layer(\n",
      "      (normalization_layer_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (multi_head_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=256, out_features=768, bias=True)\n",
      "        (fc2): Linear(in_features=768, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (normalization_layer_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (7): Transformer_Encoder_Layer(\n",
      "      (normalization_layer_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (multi_head_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=256, out_features=768, bias=True)\n",
      "        (fc2): Linear(in_features=768, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (normalization_layer_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (normalization_layer): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (classification_head): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    data_transfromation = DataTransformation(model_trainer_config)\n",
    "    data_transfromation.data_augmentation()\n",
    "\n",
    "    train_dataset, test_dataset = data_transfromation.transformed_dataset()\n",
    "\n",
    "    model_trainer = Model_trainer(model_trainer_config , train_dataset=train_dataset , test_dataset= test_dataset)\n",
    "\n",
    "    model_trainer.show_model()\n",
    "\n",
    "except Exception as e:\n",
    "  raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a03c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccbcbf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec644d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c7271f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3682f9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
